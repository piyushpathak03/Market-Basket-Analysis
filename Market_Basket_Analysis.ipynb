{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sound-cement"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "congressional-confusion"
   },
   "source": [
    "At the end of the experiment, you will be able to :\n",
    "\n",
    "* identify segments based on the overall buying behaviour\n",
    "\n",
    "* identify the association rules for the items purchased\n",
    "\n",
    "* implement the apriori algorithm from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "statistical-gregory"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exclusive-hotel"
   },
   "source": [
    "The dataset chosen for this assignment is 'Grocery Store Dataset'. The dataset contains 20 transactions of general items from a supermarket. For example, one transaction looks like 'BREAD, MILK, BISCUIT, CORNFLAKES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "southwest-active"
   },
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olympic-picture"
   },
   "source": [
    "The Apriori algorithm is an influential algorithm for searching a series of frequent sets of items in the dataset or database. It is mainly used for Association Rule mining. So, what exactly is Association Rule mining?\n",
    "\n",
    "Alex goes to buy a chips from the bakery. He grabs a Pepsi as well. The shop manager analyses that, not only Alex but people in general, often tend to buy chips and Pepsi together. After finding out the pattern, the shop manager arranges these items together and notices an increase in sales. This process of identifying the relationship between items is called association rule mining.\n",
    "\n",
    "The key concept in the Apriori algorithm is that it assumes\n",
    "\n",
    "- All subsets of a frequent itemset must be frequent\n",
    "- If an itemset is infrequent, all its supersets will be infrequent.\n",
    "\n",
    "**Important Definitions**\n",
    "\n",
    "**Itemset:** A set of items is referred as itemset and an itemset containing n items is called n-itemset.\n",
    "\n",
    "**SupportCount:** Number of transactions in  which the itemset appears.\n",
    "\n",
    "**MinimumSupportCount:** The minimum frequency of itemset in the dataset or database.\n",
    "\n",
    "**Frequent Itemset:** If an itemset satisfies minimum support, then it is a frequent itemset.\n",
    "\n",
    "**Support:** An indication of how frequently the itemset appears in the dataset..\n",
    "\n",
    "To know more about apriori algorithm click [here](https://hackr.io/blog/what-is-apriori-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "modular-volunteer"
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "forty-craps"
   },
   "source": [
    "![Overview](https://cdn.iisc.talentsprint.com/CDS/Apriori_Overview.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "documented-spiritual"
   },
   "source": [
    "We will be using below helper functions to implement the apriori algorithm:\n",
    "\n",
    "* **get_support(transactions, item_set):** This function calculates the support value for the given item_set from the provided list of transactions.\n",
    "\n",
    "* **self_join(frequent_item_sets_per_level, level):** This function performs self join in the given list of frequent itemsets of previous level, and generates the candidate itemsets for the current level.\n",
    "\n",
    "* **get_single_drop_subsets(item_set):** This function returns the subsets of the given item_set with one item less.\n",
    "\n",
    "* **is_valid_set(item_set, prev_level_sets):** This checks if the given item_set is valid, i.e., has all its subsets with support value greater than the minimum support value. It relies on the fact that prev_level_sets contains only those item_sets which are frequent, i.e., have support value greater than the minimum support value.\n",
    "\n",
    "* **pruning(frequent_item_sets_per_level, level, candidate_set):** This function performs the pruning step of the Apriori Algorithm. It takes a list candidate_set of all the candidate itemsets for the current level, and for each candidate itemset checks if all its subsets are frequent itemsets. If not, it prunes it, If yes, it adds it to the list of post_pruning_set.\n",
    "\n",
    "* **apriori(min_support):** This is the main function which uses all the above described Utility functions to implement the Apriori Algorithm and generate the list of frequent itemsets for each level for the provided transactions and min_support value.\n",
    "\n",
    "* **association_rules(min_confidence, support_dict):** This function generates the association rules in accordance with the given minimum confidence value and the provided dictionary of itemsets against their support values. It takes min_confidence and support_dict as a parameter, and returns rules as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trying-chemistry"
   },
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continent-cliff"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "controversial-tampa"
   },
   "source": [
    "\n",
    "To know about collection package click [here](https://docs.python.org/3/library/collections.html)\n",
    "\n",
    "To know about itertools package click [here](https://docs.python.org/3/library/itertools.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vertical-spirituality"
   },
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "burning-discretion"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exterior-positive"
   },
   "outputs": [],
   "source": [
    "store_data = pd.read_csv('/content/GroceryStoreDataSet.csv',names=['Items'])\n",
    "store_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "corrected-radical"
   },
   "source": [
    "A common strategy adopted by many association rule mining algorithms is to decompose the problem into two major subtasks:\n",
    "\n",
    "- Frequent Itemset Generation, whose objective is to find all the itemsets that satisfy the minimum support threshold. These itemsets are called frequent itemsets.\n",
    "- Rule Generation, whose objective is to extract all the high-confidence rules from the frequent itemsets found in the previous step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "indian-uruguay"
   },
   "source": [
    "#### Frequent Itemset Generation\n",
    "\n",
    "Create the basket with transactions\n",
    "\n",
    "To know more about lambda function click [here](https://realpython.com/python-lambda/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "light-liechtenstein"
   },
   "outputs": [],
   "source": [
    "# Creating a list of items for each order by splitting with ','\n",
    "store_data['Items'] = store_data['Items'].apply(lambda x: x.split(','))\n",
    "store_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cubic-shepherd"
   },
   "outputs": [],
   "source": [
    "# Finding the unique items from all the orders\n",
    "\n",
    "unique_items = [] # defining a empty list\n",
    "\n",
    "# Iterating over records in the dataset\n",
    "for i in store_data['Items']:\n",
    "    unique_items.extend(i) # extending the \n",
    "\n",
    "# extracting the unique items from the list by converting the list into the set\n",
    "unique_items = set(unique_items)\n",
    "unique_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rough-flight"
   },
   "source": [
    "To know about extend click [here](https://www.techbeamers.com/python-list-extend/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usual-appeal"
   },
   "outputs": [],
   "source": [
    "# Initializing an empty dictionary\n",
    "collections = dict()\n",
    "\n",
    "# Iterating over the unique items and indexing in each item\n",
    "for item in unique_items:\n",
    "    # creating a dictionary where item is the key and occurence of the item in the record is the value\n",
    "    collections[item] = [1 if item in row else 0 for row in store_data['Items'] ]\n",
    "orders = pd.DataFrame(collections)\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMksHOuJR0Ir"
   },
   "source": [
    "### Giving an index to each item to represent it with a number in each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgCfDnPSHi4k"
   },
   "outputs": [],
   "source": [
    "item_dict = dict(zip(unique_items, range(1, len(unique_items))))\n",
    "item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rental-detector"
   },
   "outputs": [],
   "source": [
    "item_list = list(orders.columns)\n",
    "item_dict = dict()\n",
    "\n",
    "# Iterating the item list\n",
    "for i, item in enumerate(item_list):\n",
    "    # Assigning index to item name to represent with a number\n",
    "    item_dict[item] = i + 1\n",
    "\n",
    "item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "successful-boards"
   },
   "source": [
    "### Identifying the pattern of the items purchased in each order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atomic-gender"
   },
   "outputs": [],
   "source": [
    "transactions = list()\n",
    "\n",
    "for i, row in orders.iterrows():\n",
    "    transaction = set()\n",
    "    \n",
    "    for item in item_dict:\n",
    "        if row[item] != 0:\n",
    "            transaction.add(item_dict[item])\n",
    "    transactions.append(transaction)\n",
    "    \n",
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sharp-electric"
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unusual-cooler"
   },
   "source": [
    "Let's create a function that return the support value for the given item_set from the provided list of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tested-invite"
   },
   "outputs": [],
   "source": [
    "def get_support(transactions, item_set):\n",
    "    match_count = 0 # initializing a variable to store the number of transactions where the given item_set is found.\n",
    "    \n",
    "    # Iterating through the the list of transactions\n",
    "    for transaction in transactions:\n",
    "        if item_set.issubset(transaction): # checking whether the given item_set is a subset of the transaction or not\n",
    "            match_count += 1 # incrementing the count when the above condition is met\n",
    "    # support value calculated by dividing the match_count by total number of transactions is returned\n",
    "    return float(match_count/len(transactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "infinite-newark"
   },
   "source": [
    "\n",
    "Let us create another helper function which performs self join in the given list of frequent itemsets of previous level and generates the candidate itemsets for the current level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "proud-pressing"
   },
   "outputs": [],
   "source": [
    "# this function takes 2 arguments as input. the 1st argument is map of level to the list of itemsets found \n",
    "# to be frequent for that level, 2nd argument is the current level number.\n",
    "def self_join(frequent_item_sets_per_level, level):\n",
    "    # initializing an empty list to store the current level candidates\n",
    "    current_level_candidates = list()\n",
    "    # Storing the list of frequent itemsets from the previous level\n",
    "    last_level_items = frequent_item_sets_per_level[level - 1]\n",
    "    \n",
    "    # If there are no frequent itemsets from the previous level, it returns an empty list for current_level_candidates. \n",
    "    # Otherwise, it iterates through each itemset in last_level_items starting from 0 for index i, \n",
    "    # and for each itemset in last_level_items starting from 1 for index j.\n",
    "    if len(last_level_items) == 0:\n",
    "        return current_level_candidates\n",
    "    \n",
    "    for i in range(len(last_level_items)):\n",
    "        for j in range(i+1, len(last_level_items)):\n",
    "            itemset_i = last_level_items[i][0]\n",
    "            itemset_j = last_level_items[j][0]\n",
    "            # union of itemsets at indices i and j\n",
    "            union_set = itemset_i.union(itemset_j)\n",
    "            \n",
    "            #If this union_set is not already present in current_level_candidates and the \n",
    "            # number of elements in the union_set is equal to the level number, \n",
    "            # then this union_set is appended into current_level_candidates.\n",
    "            if union_set not in current_level_candidates and len(union_set) == level:\n",
    "                current_level_candidates.append(union_set)\n",
    "            \n",
    "    return current_level_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "still-compound"
   },
   "source": [
    "\n",
    "We have a check for the number of elements in union_set to ensure that the current_level_candidates contain only the sets of fixed length. This is a requirement for Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recognized-investigation"
   },
   "source": [
    "Let's create another function that returns the subsets of the given items with one item less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sensitive-incentive"
   },
   "outputs": [],
   "source": [
    "def get_single_drop_subsets(item_set):\n",
    "    # initializing an empty list\n",
    "    single_drop_subsets = list()\n",
    "    \n",
    "    # Iterating over each item in the item set\n",
    "    for item in item_set:\n",
    "        # creating a temporary copy of the item_set given\n",
    "        temp = item_set.copy()\n",
    "        # removing this item from the temporary item set (a subset of length one less than the length of the item_set)\n",
    "        temp.remove(item)\n",
    "        # append this temporary set to the single_drop_subsets\n",
    "        single_drop_subsets.append(temp)\n",
    "        \n",
    "    return single_drop_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lesser-agenda"
   },
   "source": [
    "**is_valid_set()** checks if the given item_set is valid, i.e., has all its subsets with support value greater than the minimum support value. It relies on the fact that prev_level_sets contains only those item_sets which are frequent, i.e., have support value greater than the minimum support value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moving-acrobat"
   },
   "source": [
    "Now Let's create another function that checks if the given item_set is valid, i.e., has all its subsets with support value greater than the minimum support value. It relies on the fact that prev_level_sets contains only those item_sets which are frequent, i.e., have support value greater than the minimum support value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "distinct-unknown"
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_set(item_set, prev_level_sets):\n",
    "    # generating all the subsets of the given item_set with length one less than the length of the original item_set. \n",
    "    # This is done using the above described function get_single_drop_subsets() \n",
    "    single_drop_subsets = get_single_drop_subsets(item_set)\n",
    "    \n",
    "    # iterating through the single_drop_subsets list.\n",
    "    for single_drop_set in single_drop_subsets:\n",
    "        # checks if it was present in the prev_level_sets. If it wasnâ€™t it means the given \n",
    "        # item_set is a superset of a non-frequent item_set. Thus, it returns False\n",
    "        # If all the single_drop_subsets are frequent itemsets, and are present in the prev_level_sets, it returns True\n",
    "        if single_drop_set not in prev_level_sets:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interior-tunisia"
   },
   "source": [
    "Now let's perform the pruning step of the Apriori Algorithm. It takes a list candidate_set of all the candidate itemsets for the current level, and for each candidate itemset checks if all its subsets are frequent itemsets. If not, it prunes it, If yes, it adds it to the list of post_pruning_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "textile-oklahoma"
   },
   "outputs": [],
   "source": [
    "def pruning(frequent_item_sets_per_level, level, candidate_set):\n",
    "    # Initializing empty list to store the list of frequent itemsets for the current level \n",
    "    # after performing pruning operation on the given list of candidate sets.\n",
    "    post_pruning_set = list()\n",
    "    # If there are no candidate_set, it returns an empty list. \n",
    "    # Otherwise, it first creates a list of frequent itemsets from the previous level and stores it in prev_level_sets.\n",
    "    if len(candidate_set) == 0:\n",
    "        return post_pruning_set\n",
    "    \n",
    "    prev_level_sets = list()\n",
    "    for item_set, _ in frequent_item_sets_per_level[level - 1]:\n",
    "        prev_level_sets.append(item_set)\n",
    "    \n",
    "    # Iterating over each item_set in candidate_set list\n",
    "    for item_set in candidate_set:\n",
    "        # checking whether it is a valid itemset or not\n",
    "        if is_valid_set(item_set, prev_level_sets):\n",
    "            # If this item_set is valid, it is appended to the list of post_pruning_set.\n",
    "            post_pruning_set.append(item_set)\n",
    "            \n",
    "    return post_pruning_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "powerful-cannon"
   },
   "source": [
    "### Apriori algorithm\n",
    "\n",
    "Now let us use all the above defined helper functions to implement the Apriori Algorithm and generate the list of frequent itemsets for each level for the provided transactions and min_support value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joint-zimbabwe"
   },
   "outputs": [],
   "source": [
    "def apriori(min_support):\n",
    "    # creating a default empty dictionary which maps level numbers to the list of frequent itemsets for that level\n",
    "    frequent_item_sets_per_level = defaultdict(list)\n",
    "    print(\"level : 1\", end = \" \")\n",
    "    \n",
    "    # iterating through the list of all items item_list\n",
    "    for item in range(1, len(item_list) + 1):\n",
    "        # calculate the support value of each item using the helper function get_support(). \n",
    "        # If this support value is greater than or equal to the provided min_support value, \n",
    "        # this item_set is added to the list of frequent itemsets for this level.\n",
    "        support = get_support(transactions, {item})\n",
    "        if support >= min_support:\n",
    "            # every itemset is stored as a pair of 2 values: item, support\n",
    "            frequent_item_sets_per_level[1].append(({item}, support))\n",
    "            \n",
    "    # For each level greater than 1, generate the current_level_candidates itemsets \n",
    "    # by performing self_join() on the frequent itemsets of the previous level.     \n",
    "    for level in range(2, len(item_list) + 1):\n",
    "        print(level, end = \" \")\n",
    "        current_level_candidates = self_join(frequent_item_sets_per_level, level)\n",
    "        \n",
    "        # perform the pruning operation on these current_level_candidates using the pruning() \n",
    "        # helper function defined above, and obtain the results in post_pruning_candidates\n",
    "        post_pruning_candidates = pruning(frequent_item_sets_per_level, level, current_level_candidates)\n",
    "        \n",
    "        # if there is no itemset left after pruning, we break the loop. \n",
    "        # It means there is no point in processing for further levels. \n",
    "        # Otherwise, for each item_set in post_pruning_candidates, \n",
    "        # we calculate the support value using the get_support() helper function.\n",
    "        if len(post_pruning_candidates) == 0:\n",
    "            break\n",
    "        \n",
    "        for item_set in post_pruning_candidates:\n",
    "            support = get_support(transactions, item_set)\n",
    "            # If this support value is greater than or equal to the given min_support, \n",
    "            # we append this item_set into the list of frequent itemsets for this level.\n",
    "            if support >= min_support:\n",
    "                frequent_item_sets_per_level[level].append((item_set, support))\n",
    "                \n",
    "    return frequent_item_sets_per_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rocky-illness"
   },
   "outputs": [],
   "source": [
    "# defining the minimum support value as 0.005\n",
    "min_support = 0.005\n",
    "frequent_item_sets_per_level = apriori(min_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alleged-coalition"
   },
   "outputs": [],
   "source": [
    "for level in frequent_item_sets_per_level:\n",
    "    print(len(frequent_item_sets_per_level[level]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dress-assist"
   },
   "outputs": [],
   "source": [
    "for level in frequent_item_sets_per_level:\n",
    "    print(frequent_item_sets_per_level[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nutritional-subcommittee"
   },
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps items to their support values.\n",
    "item_support_dict = dict()\n",
    "item_list = list() # to store the name of items corresponding to item_dict values retrieved from frequent_item_sets_per_level\n",
    "\n",
    "# Keys and values are retrieved from the item_dict and stored in separate variables\n",
    "key_list = list(item_dict.keys())\n",
    "val_list = list(item_dict.values())\n",
    "\n",
    "# For each level in frequent_item_sets_per_level, for each item-support pair, name of the item retrieved from the key_list \n",
    "# that corresponds to the number in set_support_pair, and names are added to the item_list.\n",
    "for level in frequent_item_sets_per_level:\n",
    "    for set_support_pair in frequent_item_sets_per_level[level]:\n",
    "        for i in set_support_pair[0]:\n",
    "            item_list.append(key_list[val_list.index(i)])\n",
    "        # Items names and their support values are mapped in the item_support_dict as a frozenset-float number pair.\n",
    "        item_support_dict[frozenset(item_list)] = set_support_pair[1]\n",
    "        item_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "considerable-editing"
   },
   "outputs": [],
   "source": [
    "item_support_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "super-morocco"
   },
   "source": [
    "**find_possible_subsets()** takes each item from the item_support_dict and its length item_length as parameter, and returns all possible combinations of elements inside the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rapid-birthday"
   },
   "outputs": [],
   "source": [
    "def find_possible_subsets(item, item_length):\n",
    "    # creating empty list to store a list of combinations.\n",
    "    combs = []\n",
    "    \n",
    "    # Iterating over the items\n",
    "    for i in range(1, item_length + 1):\n",
    "        # appending a list of all possible combinations of items to the combs array.\n",
    "        combs.append(list(combinations(item, i)))\n",
    "    \n",
    "    # Creating a subset array\n",
    "    subsets = []\n",
    "    for comb in combs:\n",
    "        for elt in comb:\n",
    "            subsets.append(elt)\n",
    "            \n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "national-alexandria"
   },
   "source": [
    "### Generate the Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFrvVH0iHG_V"
   },
   "outputs": [],
   "source": [
    "item = {1,2,3}\n",
    "b = item.difference({1,2,3})\n",
    "if b:\n",
    "  print(\"Do something\")\n",
    "item | b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "commercial-volume"
   },
   "outputs": [],
   "source": [
    "def association_rules(min_confidence, support_dict):\n",
    "    rules = list()\n",
    "    \"\"\"For itemsets of more than one element, it first finds all their subsets calling the find_subset(item, item_length)\n",
    "        For every subset A, it calculates the set B = itemset-A.\n",
    "        If B is not empty, the confidence of B is calculated.\n",
    "        If this value is more than minimum confidence value, the rule A->B is added to the list rules with the corresponding confidence value of B.\"\"\"\n",
    "    for item, support in support_dict.items():\n",
    "        item_length = len(item)\n",
    "       \n",
    "        if item_length > 1:\n",
    "            subsets = find_possible_subsets(item, item_length)\n",
    "           \n",
    "            for A in subsets:\n",
    "                B = item.difference(A)\n",
    "                if B:\n",
    "                    A = frozenset(A)\n",
    "                    \n",
    "                    AB = A | B\n",
    "                    \n",
    "                    confidence = support_dict[AB] / support_dict[A]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((A, B, confidence))\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "formal-empty"
   },
   "outputs": [],
   "source": [
    "association_rules = association_rules(min_confidence = 0.6, support_dict = item_support_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suburban-trout"
   },
   "outputs": [],
   "source": [
    "print(\"Number of rules: \", len(association_rules), \"\\n\")\n",
    "\n",
    "for rule in association_rules:\n",
    "    print('{0} -> {1} <confidence: {2}>'.format(set(rule[0]), set(rule[1]), rule[2]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
